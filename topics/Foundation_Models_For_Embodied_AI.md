# Foundation Models for Embodied AI

## Affordance

* **[ICCV 21]** Where2Act: From Pixels to Actions for Articulated 3D Objects, [website](https://cs.stanford.edu/~kaichun/where2act/)

* **[CVPR 23]** Affordance Diffusion: Synthesizing Hand-Object Interactions, [website](https://judyye.github.io/affordiffusion-www/)

* **[ICCV 23]** AffordPose: A Large-scale Dataset of Hand-Object Interactions with Affordance-driven Hand Pose, [website](https://affordpose.github.io/)

* **[arXiv 24]** Robo-ABC: Affordance Generalization Beyond Categories via Semantic Correspondence for Robot Manipulation, [arXiv](https://arxiv.org/abs/2401.07487)

* **[CVPR 22]** Joint Hand Motion and Interaction Hotspots Prediction from Egocentric Videos, [website](https://stevenlsw.github.io/hoi-forecast/)

* **[CVPR 19]** ContactGrasp: Functional Multi-finger Grasp Synthesis from Contact, [website](https://contactdb.cc.gatech.edu/contactgrasp.html)

* **[3DV 20]** Grasping Field: Learning Implicit Representations for Human Grasps, [PDF](https://arxiv.org/pdf/2008.04451)

* **[IROS 24(Oral)]** ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models, [website](https://github.com/SiyuanHuang95/ManipVQA)

* **[CVPR 23]** VRB: Affordances from Human Videos as a Versatile Representation for Robotics, [website](https://robo-affordances.github.io/)

* **[arXiv 24]** Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise: https://arxiv.org/pdf/2402.18699

* **[arXiv 24]** General Flow as Foundation Affordance for Scalable Robot Learning, [website](https://general-flow.github.io/)

* **[ICCV 23]** ContactGen: Generative Contact Modeling for Grasp Generation, [website](https://stevenlsw.github.io/contactgen/)

----


* **[ICLR 22]** VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects, [website](https://hyperplane-lab.github.io/vat-mart/)

* **[arXiv 24]** PreAfford: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments, [arXiv](https://air-discover.github.io/PreAfford/)

* **[ICLR 23]** DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation, [arXiv](https://arxiv.org/abs/2207.01971)

* **[NIPS 23]** Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects



https://arxiv.org/pdf/2406.07549




* CLIP: Zero-shot Jack of All Trades, [website](https://blog.kzakka.com/posts/clip/), [CLIP GradCAM CLIP_GradCAM_Visualization](https://colab.research.google.com/github/kevinzakka/clip_playground/blob/main/CLIP_GradCAM_Visualization.ipynb)







## Tracking & Estimation

* **[CVPR 20(Oral)]** Understanding Human Hands in Contact at Internet Scale, [website](https://fouheylab.eecs.umich.edu/~dandans/projects/100DOH/)

* **[CVPR 24]** TAPIR: Towards Spatial Intelligence via Point Tracking, [website](https://deepmind-tapir.github.io/blogpost.html)


## Correspondance

* **[NIPS 23]** Emergent Correspondence from Image Diffusion, [website](https://diffusionfeatures.github.io/)

## World Model

* **[post 24]** Genie 2: A large-scale foundation world model, [website](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)